{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49dbb19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chait\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 4098 images belonging to 4 classes.\n",
      "Found 1023 images belonging to 4 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\chait\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\chait\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\chait\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\chait\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\chait\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "129/129 [==============================] - 368s 3s/step - loss: -1051046336.0000 - accuracy: 0.0105 - val_loss: -8052660736.0000 - val_accuracy: 0.0098\n",
      "Epoch 2/5\n",
      "129/129 [==============================] - 321s 2s/step - loss: -167213039616.0000 - accuracy: 0.0102 - val_loss: -613231820800.0000 - val_accuracy: 0.0098\n",
      "Epoch 3/5\n",
      "129/129 [==============================] - 295s 2s/step - loss: -2722535571456.0000 - accuracy: 0.0102 - val_loss: -6690422915072.0000 - val_accuracy: 0.0098\n",
      "Epoch 4/5\n",
      "129/129 [==============================] - 290s 2s/step - loss: -16905178972160.0000 - accuracy: 0.0102 - val_loss: -33098836213760.0000 - val_accuracy: 0.0098\n",
      "Epoch 5/5\n",
      "129/129 [==============================] - 301s 2s/step - loss: -63801441386496.0000 - accuracy: 0.0102 - val_loss: -108751524724736.0000 - val_accuracy: 0.0098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x271ce756d50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#define image size and batch size\n",
    "IMG_SIZE=224\n",
    "BATCH_SIZE=32\n",
    "#creating training data\n",
    "train_datagen =ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    "\n",
    ")\n",
    "#creating training data with above parameters\n",
    "\n",
    "#folder=parameters.flow_from_directory(path,ts,bs,cm,subset)\n",
    "train_generator=train_datagen.flow_from_directory(\n",
    "r\"C:\\Users\\chait\\Downloads\\archive (3)\\Alzheimer_s Dataset\\train\",\n",
    "target_size=(IMG_SIZE,IMG_SIZE),\n",
    "batch_size=BATCH_SIZE,\n",
    "class_mode='binary',\n",
    "subset='training'\n",
    ")\n",
    "#creating validation data\n",
    "valid_generator=train_datagen.flow_from_directory(\n",
    "r\"C:\\Users\\chait\\Downloads\\archive (3)\\Alzheimer_s Dataset\\train\",\n",
    "target_size=(IMG_SIZE,IMG_SIZE),\n",
    "batch_size=BATCH_SIZE,\n",
    "class_mode='binary',\n",
    "subset='validation'\n",
    ")\n",
    "#define the model\n",
    "model=keras.Sequential([\n",
    "    layers.Conv2D(32,(3,3),activation='relu',input_shape=(IMG_SIZE,IMG_SIZE,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64,(3,3),activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128,(3,3),activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128,activation='relu'),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "#compile the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "#fit the model\n",
    "model.fit(train_generator,validation_data=valid_generator,epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc39659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chait\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Alzheimer.h5\",\"label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0e93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "#load the saved model\n",
    "model=load_model(r\"C:\\Users\\chait\\Downloads\\Alzheimer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ca6e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 297ms/step\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "test_image_path=r\"C:\\Users\\chait\\Downloads\\archive (3)\\Alzheimer_s Dataset\\train\\NonDemented\\nonDem996.jpg\"\n",
    "img=image.load_img(test_image_path,target_size=(224,224))\n",
    "img_array=image.img_to_array(img)\n",
    "img_array=np.expand_dims(img_array,axis=0)\n",
    "#Add batch dimension\n",
    "img_array/=255. #Normalize pixel values\n",
    "#Make predictions\n",
    "prediction=model.predict(img_array)\n",
    "#print prediction\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5399b2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Not having Alzheimer\n"
     ]
    }
   ],
   "source": [
    "if prediction < 0.5:\n",
    "  print(\"Having Alzheimer \") \n",
    "else:\n",
    "  print(\" Not having Alzheimer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
